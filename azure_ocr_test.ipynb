{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.ai.formrecognizer import DocumentModelAdministrationClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from docx import Document\n",
    "from pyresparser import ResumeParser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configurar el cliente de Form Recognizer\n",
    "endpoint = os.getenv(\"AZURE_FORM_RECOGNIZER_ENDPOINT\")\n",
    "key = os.getenv(\"AZURE_FORM_RECOGNIZER_KEY\")\n",
    "model_id = \"model_1\"\n",
    "\n",
    "credential = AzureKeyCredential(key)\n",
    "document_model_admin_client = DocumentModelAdministrationClient(endpoint, credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def ocr_analysis(doc_name):\n",
    "\n",
    "#     with open(doc_name + '.pdf', \"rb\") as fd:\n",
    "#         document = fd.read()\n",
    "    \n",
    "#     document_analysis_client = DocumentAnalysisClient(\n",
    "#         endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "    \n",
    "#     poller = document_analysis_client.begin_analyze_document (\"prebuilt-read\", document)\n",
    "#     result = poller.result()\n",
    "    \n",
    "#     # Save dictionary to JSON file\n",
    "#     with open(doc_name + '.json', 'w') as json_file:\n",
    "#         json.dump(result.to_dict(), json_file)\n",
    "    \n",
    "#     # Load JSON file and extract content to text file\n",
    "#     with open(doc_name + '.json', 'r') as json_file:\n",
    "#         pdf = json.load(json_file)\n",
    "    \n",
    "#     with open(doc_name + '.txt', 'w') as text_file:\n",
    "#         # Write the contents of the dictionary to the text file\n",
    "#         text_file.write(str(pdf['content']))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_analysis(doc_name):\n",
    "    try:\n",
    "        with open(doc_name, \"rb\") as fd:\n",
    "            document = fd.read()\n",
    "        \n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "        \n",
    "        poller = document_analysis_client.begin_analyze_document (\"prebuilt-read\", document)\n",
    "        result = poller.result()\n",
    "        \n",
    "        # Extract text from OCR result\n",
    "        extracted_text = str(result.content)\n",
    "        \n",
    "        return {'status': 'success', 'extracted_text': extracted_text}\n",
    "    except Exception as e:\n",
    "        return {'status': 'error', 'message': str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'extracted_text': 'Kalash Jindal\\n3rd year B Tech, Computer Science and Engineering Feroze Gandhi Institute of Engineering and Technology\\nEDUCATION\\n8.71/10.0 CGPA (Up to 2nd year) Feroze Gandhi Institute Of Engineering and Technology, Raebareli — B tech Computer Science and Engineering 2017 - Till Date\\n84.4/100 Percentage Agarwal Public Inter College, Sitapur — Intermediate 2016\\n298, Thomsanganj, Sitapur Uttar Pradesh, India +91-7839453651 jindalkalash298@gmail.com www.linkedin.com/in/kalashj16/\\nSKILLS\\nPython, Statistics, Data Handling, Data Visualization, Linear Algebra, Neural Networks, Transfer Learning, Feature Extraction, Deep Learning, Sci-kit Learn, Keras, OpenCV, SQLite, Html, CSS, GUI using Pyqt module, Git GitHub, C, C++, etc.\\nHOBBIES Table Tennis, Cooking, Reading Books, Web Surfing, etc.\\n90.1/100 Percentage Agarwal Public Inter College, Sitapur — High School 2014\\nTRAINING\\nMachine Learning Master Course — Coding Blocks https://online.codingblocks.com/app/certificates/CBOL-21111-b988\\nDivide and Conquer, Sorting and Searching, and Randomized Algorithms — Coursera https://www.coursera.org/account/accomplishments/certificate/JS7Q AT5HYYKG\\nIntroduction to Data Science in Python — Coursera https://www.coursera.org/account/accomplishments/verify/ANH8FAC ZZHE8\\nProgramming with Python — Internshala Trainings Certificate Number : A20916E5-8524-A7B2-091A-EC4CE0C2CA4F\\nPROJECTS\\nCartpole Playing Game using Reinforcement Learning A Cartpole (prebuild API by OpenAI GYM) is placed in the one-dimensional track having a pole which can move either left or right. The goal is to make the left and the right momentum in such a way that the pole does not fall down from a certain angle. So, I have used the GYM library for importing the Cartpole model and Q- learning principle to design the algorithm for constantly making momentum of the pole to achieve the highest accuracy.\\nSentimental Analysis using MLP and LSTM on IMDB Dataset\\nIn this project, I have worked on the IMDB dataset for checking the sentiment of the reviews given by the reviewers either the review is positive or negative which to lead to giving the movie rating out of 5.. I have also implemented the concept of early stopping, to overcome the overfitting problem.\\nEmoji Prediction using LSTM\\nThis project is based on sensing the right emoji from the set of emoji according to the sentiment of the piece of line. While modeling the neural network, I have used LSTM for it with activation function softmax and GloVe ( Global Vector For Word representation), which is an unsupervised learning algorithm for obtaining the vector representation of words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. Adam is used for optimizing the model, the loss is categorical cross-entropy.\\nText Generation using Markov Chain\\nIn this project, I have worked on a probabilistic model for text/natural language generation. Using the Markov Chain algorithm for generating the text upto a certain length.\\nImage Classification using SVM\\nIn this project, I have worked on support vector machine for extracting the features from the training dataset containing images of dog, cat, horse and humans of around 800. Applying data augmentation for increasing the size of dataset and using SVM for classifying the testing set into 4 classes.\\nOdd one out\\nThis project works to find out odd one out, i.e, the least similar element out from any inputted list. Word2Vec model using Gensim(pre-trained model)is used for word embedding(vector representation of the words)\\nBollywood Word Analogies\\nIn this project, I have worked with Word2Vec model for finding the analogies of word, i.e, used to complete the sentence “a is to b and c is to __ ” . The goal is to find out the correct word, I have measured the cosine similarity of the word vectors of (b-a) and (v-c). Here, v is the list of word vectors present in the vocab of word2vec model.\\nTitanic Survivor Prediction using Decision Trees —\\nTitanic Survivor is the very first question on kaggle. In this problem, I have used decision trees for predicting whether a person will survive or not on the testing data.\\nDiabetics Detection\\nThis project is modeled using the KNN and the dataset used is of people of any country having high diabetics patients and the goal is to find out that the patient is diabetic or not on testing data, I have used binary classifier for classifying the possibility.\\nFantasy Cricket Game\\nThis game is a prototype of dream 11. I have work on this project while completing my python training. In this project, I have used python, SQLite for database management and qt designer for UI designing.\\nAir Quality Prediction\\nDataset used in this project is of the different places with having 5 features. So, I have used Linear regression with multiple features for finding out the air quality of testing data and at last, I achieve an accuracy of 98%.\\nHard Work Pays Off\\nThis project is based on the data of the students and how they performed in the evaluation exam. And I have implemented Linear Regression for finding out the minimum time required for each student to do coding daily. R2 Score is used for calculating the accuracy and I got 99% accuracy.\\nSeparating Chemicals\\nIn this project, I have worked on logistic regression to create a model to solve this challenge. Dataset is labeled with 0 and 1 with each having 3 features and moto is to label the testing data. I attain 96% accuracy.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr = ocr_analysis('CV.pdf')\n",
    "ocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Kalash Jindal\\\\n3rd year B Tech, Computer Science and Engineering Feroze Gandhi Institute of Engineering and Technology\\\\nEDUCATION\\\\n8.71/10.0 CGPA (Up to 2nd year) Feroze Gandhi Institute Of Engineering and Technology, Raebareli \\\\u2014 B tech Computer Science and Engineering 2017 - Till Date\\\\n84.4/100 Percentage Agarwal Public Inter College, Sitapur \\\\u2014 Intermediate 2016\\\\n298, Thomsanganj, Sitapur Uttar Pradesh, India +91-7839453651 jindalkalash298@gmail.com www.linkedin.com/in/kalashj16/\\\\nSKILLS\\\\nPython, Statistics, Data Handling, Data Visualization, Linear Algebra, Neural Networks, Transfer Learning, Feature Extraction, Deep Learning, Sci-kit Learn, Keras, OpenCV, SQLite, Html, CSS, GUI using Pyqt module, Git GitHub, C, C++, etc.\\\\nHOBBIES Table Tennis, Cooking, Reading Books, Web Surfing, etc.\\\\n90.1/100 Percentage Agarwal Public Inter College, Sitapur \\\\u2014 High School 2014\\\\nTRAINING\\\\nMachine Learning Master Course \\\\u2014 Coding Blocks https://online.codingblocks.com/app/certificates/CBOL-21111-b988\\\\nDivide and Conquer, Sorting and Searching, and Randomized Algorithms \\\\u2014 Coursera https://www.coursera.org/account/accomplishments/certificate/JS7Q AT5HYYKG\\\\nIntroduction to Data Science in Python \\\\u2014 Coursera https://www.coursera.org/account/accomplishments/verify/ANH8FAC ZZHE8\\\\nProgramming with Python \\\\u2014 Internshala Trainings Certificate Number : A20916E5-8524-A7B2-091A-EC4CE0C2CA4F\\\\nPROJECTS\\\\nCartpole Playing Game using Reinforcement Learning A Cartpole (prebuild API by OpenAI GYM) is placed in the one-dimensional track having a pole which can move either left or right. The goal is to make the left and the right momentum in such a way that the pole does not fall down from a certain angle. So, I have used the GYM library for importing the Cartpole model and Q- learning principle to design the algorithm for constantly making momentum of the pole to achieve the highest accuracy.\\\\nSentimental Analysis using MLP and LSTM on IMDB Dataset\\\\nIn this project, I have worked on the IMDB dataset for checking the sentiment of the reviews given by the reviewers either the review is positive or negative which to lead to giving the movie rating out of 5.. I have also implemented the concept of early stopping, to overcome the overfitting problem.\\\\nEmoji Prediction using LSTM\\\\nThis project is based on sensing the right emoji from the set of emoji according to the sentiment of the piece of line. While modeling the neural network, I have used LSTM for it with activation function softmax and GloVe ( Global Vector For Word representation), which is an unsupervised learning algorithm for obtaining the vector representation of words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. Adam is used for optimizing the model, the loss is categorical cross-entropy.\\\\nText Generation using Markov Chain\\\\nIn this project, I have worked on a probabilistic model for text/natural language generation. Using the Markov Chain algorithm for generating the text upto a certain length.\\\\nImage Classification using SVM\\\\nIn this project, I have worked on support vector machine for extracting the features from the training dataset containing images of dog, cat, horse and humans of around 800. Applying data augmentation for increasing the size of dataset and using SVM for classifying the testing set into 4 classes.\\\\nOdd one out\\\\nThis project works to find out odd one out, i.e, the least similar element out from any inputted list. Word2Vec model using Gensim(pre-trained model)is used for word embedding(vector representation of the words)\\\\nBollywood Word Analogies\\\\nIn this project, I have worked with Word2Vec model for finding the analogies of word, i.e, used to complete the sentence \\\\u201ca is to b and c is to __ \\\\u201d . The goal is to find out the correct word, I have measured the cosine similarity of the word vectors of (b-a) and (v-c). Here, v is the list of word vectors present in the vocab of word2vec model.\\\\nTitanic Survivor Prediction using Decision Trees \\\\u2014\\\\nTitanic Survivor is the very first question on kaggle. In this problem, I have used decision trees for predicting whether a person will survive or not on the testing data.\\\\nDiabetics Detection\\\\nThis project is modeled using the KNN and the dataset used is of people of any country having high diabetics patients and the goal is to find out that the patient is diabetic or not on testing data, I have used binary classifier for classifying the possibility.\\\\nFantasy Cricket Game\\\\nThis game is a prototype of dream 11. I have work on this project while completing my python training. In this project, I have used python, SQLite for database management and qt designer for UI designing.\\\\nAir Quality Prediction\\\\nDataset used in this project is of the different places with having 5 features. So, I have used Linear regression with multiple features for finding out the air quality of testing data and at last, I achieve an accuracy of 98%.\\\\nHard Work Pays Off\\\\nThis project is based on the data of the students and how they performed in the evaluation exam. And I have implemented Linear Regression for finding out the minimum time required for each student to do coding daily. R2 Score is used for calculating the accuracy and I got 99% accuracy.\\\\nSeparating Chemicals\\\\nIn this project, I have worked on logistic regression to create a model to solve this challenge. Dataset is labeled with 0 and 1 with each having 3 features and moto is to label the testing data. I attain 96% accuracy.\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_string = json.dumps(ocr['extracted_text'])\n",
    "ocr_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DooFromash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# download stopwords if necessary\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# define stopwords list\n",
    "stop_words = set(stopwords.words('english', 'spanish'))\n",
    "\n",
    "# define function to clean text\n",
    "def clean_text(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # remove special characters, punctuation, and emojis\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    # tokenize into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # join the words back into a string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(ocr_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import emoji\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_skills(text):\n",
    "    skills = []\n",
    "    doc = nlp(text.lower())\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.root.pos_ == \"NOUN\":\n",
    "            for token in chunk:\n",
    "                if token.pos_ == \"ADJ\" or token.pos_ == \"NOUN\":\n",
    "                    skills.append(token.text)\n",
    "    return list(set(skills))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['global',\n",
       " 'person',\n",
       " 'evaluation',\n",
       " 'crossentropyntext',\n",
       " 'course',\n",
       " 'accuracy',\n",
       " 'loss',\n",
       " 'images',\n",
       " 'patients',\n",
       " 'feature',\n",
       " 'decision',\n",
       " 'places',\n",
       " 'engineering',\n",
       " 'classifier',\n",
       " 'dataset',\n",
       " 'lead',\n",
       " 'humans',\n",
       " 'correct',\n",
       " 'element',\n",
       " 'works',\n",
       " 'representation',\n",
       " 'reviewers',\n",
       " 'diabetic',\n",
       " 'highest',\n",
       " 'word2vec',\n",
       " 'certificate',\n",
       " 'question',\n",
       " 'principle',\n",
       " 'game',\n",
       " 'country',\n",
       " 'model',\n",
       " 'generation',\n",
       " 'classification',\n",
       " 'people',\n",
       " 'work',\n",
       " 'tech',\n",
       " 'categorical',\n",
       " 'daily',\n",
       " 'different',\n",
       " 'visualization',\n",
       " 'concept',\n",
       " 'features',\n",
       " 'similar',\n",
       " 'data',\n",
       " 'vector',\n",
       " 'network',\n",
       " 'embeddingvector',\n",
       " 'database',\n",
       " 'trees',\n",
       " 'management',\n",
       " 'diabetics',\n",
       " 'analysis',\n",
       " 'support',\n",
       " 'dream',\n",
       " 'rating',\n",
       " 'table',\n",
       " 'extracting',\n",
       " 'machine',\n",
       " 'movie',\n",
       " 'scikit',\n",
       " 'training',\n",
       " 'prediction',\n",
       " 'pole',\n",
       " 'piece',\n",
       " 'line',\n",
       " 'networks',\n",
       " 'regression',\n",
       " 'wordsnbollywood',\n",
       " 'master',\n",
       " 'angle',\n",
       " 'accuracynseparating',\n",
       " 'certain',\n",
       " 'negative',\n",
       " 'deep',\n",
       " 'classesnodd',\n",
       " 'horse',\n",
       " 'student',\n",
       " 'score',\n",
       " 'air',\n",
       " 'logistic',\n",
       " 'designer',\n",
       " 'least',\n",
       " 'linear',\n",
       " 'quality',\n",
       " 'patient',\n",
       " 'trainings',\n",
       " 'word',\n",
       " 'sqlite',\n",
       " 'accuracynsentimental',\n",
       " 'text',\n",
       " 'lengthnimage',\n",
       " 'ie',\n",
       " 'markov',\n",
       " 'algebra',\n",
       " 'words',\n",
       " 'modelis',\n",
       " 'students',\n",
       " 'high',\n",
       " 'textnatural',\n",
       " 'goal',\n",
       " 'predictionndataset',\n",
       " 'analogiesnin',\n",
       " 'outnthis',\n",
       " 'sentiment',\n",
       " 'design',\n",
       " 'language',\n",
       " 'vectors',\n",
       " 'neural',\n",
       " 'problemnemoji',\n",
       " 'list',\n",
       " 'extraction',\n",
       " 'augmentation',\n",
       " 'cat',\n",
       " 'size',\n",
       " 'computer',\n",
       " 'datandiabetics',\n",
       " 'r2',\n",
       " 'gym',\n",
       " 'reviews',\n",
       " 'exam',\n",
       " 'positive',\n",
       " 'binary',\n",
       " 'problem',\n",
       " 'algorithm',\n",
       " 'chain',\n",
       " 'learning',\n",
       " 'library',\n",
       " 'survivor',\n",
       " 'science',\n",
       " 'momentum',\n",
       " 'project',\n",
       " 'testing',\n",
       " 'number',\n",
       " 'python']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "skills = extract_skills(cleaned_text)\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from ftfy import fix_text\n",
    "\n",
    "def ngrams(string, n=3):\n",
    "    string = fix_text(string) # fix text\n",
    "    string = string.encode(\"ascii\", errors=\"ignore\").decode() #remove non ascii chars\n",
    "    string = string.lower()\n",
    "    chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n",
    "    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "    string = re.sub(rx, '', string)\n",
    "    string = string.replace('&', 'and')\n",
    "    string = string.replace(',', ' ')\n",
    "    string = string.replace('-', ' ')\n",
    "    string = string.title() # normalise case - capital at start of each word\n",
    "    string = re.sub(' +',' ',string).strip() # get rid of multiple spaces and replace with a single\n",
    "    string = ' '+ string +' ' # pad names for ngrams...\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase=False)\n",
    "tfidf = vectorizer.fit_transform(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stopw  = set(stopwords.words('english'))\n",
    "df =pd.read_csv('job_final.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       About company: Smart Food Safe Solutions Inc. ...\n",
       "1       Location Bangalore Experience Years Job Descri...\n",
       "2       Open Systems International, Inc. (OSI) www.osi...\n",
       "3       About Job Software Testing Engineer Job Descri...\n",
       "4       Location: Bangalore Experience: 6Years Skills ...\n",
       "                              ...                        \n",
       "1919    Skills Qualifications: Years experience Strong...\n",
       "1920    Job TH10519_13189 Posted on: 29th May, 2019Job...\n",
       "1921    Job Description spend percent lives buildings....\n",
       "1922    (Job Number: 1905027) Job Title â€“ Web Develo...\n",
       "1923    marry design engineering language ways produce...\n",
       "Name: test, Length: 1924, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['test']=df['Job_Description'].apply(lambda x: ' '.join([word for word in str(x).split() if len(word)>2 and word not in (stopw)]))\n",
    "df['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)\n",
    "test = (df['test'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getNearestN(query):\n",
    "  queryTFIDF_ = vectorizer.transform(query)\n",
    "  distances, indices = nbrs.kneighbors(queryTFIDF_)\n",
    "  return distances, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = getNearestN(test)\n",
    "test = list(test) \n",
    "matches = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = getNearestN(test)\n",
    "test = list(test) \n",
    "matches = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mmatch\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mmatches[\u001b[39m'\u001b[39;49m\u001b[39mMatch confidence\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m      2\u001b[0m df1\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39msort_values(\u001b[39m'\u001b[39m\u001b[39mmatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df1[[\u001b[39m'\u001b[39m\u001b[39mPosition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCompany\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mLocation\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmatch\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mreset_index()\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "df['match']=matches['Match confidence']\n",
    "df1=df.sort_values('match')\n",
    "df1[['Position', 'Company','Location','match']].head(10).reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f5-nlp-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
